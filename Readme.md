ğŸ” Overview

This project implements a Retrieval-Augmented Generation (RAG) system that allows users to ask questions about YouTube video content.
https://youtu.be/vLqTf2b6GZw?si=s8g28ArYdKdKH7ts this is the link of the video it is apna college python tutorial.

The system:

Converts YouTube videos into audio

Transcribes audio into text

Splits the text into meaningful chunks

Generates vector embeddings for each chunk

Stores embeddings in a vector database

Retrieves the most relevant chunks using cosine similarity

Uses a Large Language Model (LLM) to generate accurate answers

This enables context-aware question answering directly from video content.

ğŸ§  Why RAG?

Large Language Models alone can hallucinate or miss video-specific details.
RAG solves this by:

Retrieving relevant video content first

Then generating answers grounded in that content

ğŸ› ï¸ Technologies Used
ğŸ”¹ Language & Libraries

Python

Pandas

NumPy

scikit-learn

joblib

requests

ğŸ”¹ Embedding Model

bge-m3
Used to convert text chunks into dense vector embeddings.

ğŸ”¹ Similarity Search

Cosine Similarity (from sklearn.metrics.pairwise)

ğŸ”¹ LLM

LLaMA 3.2 (served locally using Ollama API)

ğŸ”¹ Vector Storage

Embeddings stored using joblib for fast loading and retrieval.

ğŸ”„ Project Workflow
1ï¸âƒ£ YouTube Video â†’ Audio

The YouTube video is converted into an audio file.

2ï¸âƒ£ Audio â†’ Text

Audio is transcribed into text using a speech-to-text process.

3ï¸âƒ£ Text Chunking

The transcription is split into smaller chunks to preserve semantic meaning.

4ï¸âƒ£ Embedding Creation

Each chunk is converted into a vector embedding using the bge-m3 model.

def create_embedding(text_list):
    r = requests.post("http://localhost:11434/api/embed", json={
        "model": "bge-m3",
        "input": text_list
    })
    return r.json()["embeddings"]

5ï¸âƒ£ Vector Database

All embeddings are stored along with metadata (title, timestamps, text).

df = joblib.load("embeddings.joblib")

6ï¸âƒ£ Retrieval Using Cosine Similarity

User questions are embedded and matched against stored vectors.

similarities = cosine_similarity(
    np.vstack(df['embedding'].values),
    [question_embedding]
).flatten()


Top relevant chunks are selected.

7ï¸âƒ£ Answer Generation (RAG)

Retrieved chunks are passed to the LLM to generate a grounded answer.

def inference(prompt):
    r = requests.post("http://localhost:11434/api/generate", json={
        "model": "llama3.2",
        "prompt": prompt,
        "stream": False
    })
    return r.json()

ğŸ§ª Example Use Case

Ask: â€œWhere is gradient descent explained in the video?â€

System:

Retrieves relevant timestamps

Responds with video number + exact time

Prevents unrelated questions

ğŸš€ Key Features

âœ”ï¸ Local LLM (No paid API)
âœ”ï¸ Semantic search on video content
âœ”ï¸ Timestamp-aware answers
âœ”ï¸ Scalable chunk-based retrieval
âœ”ï¸ Practical RAG implementation

ğŸ“‚ Repository Structure (High Level)
RAG-Model/
â”‚
â”œâ”€â”€ audio_processing/           
â”œâ”€â”€ text_chunking/
â”œâ”€â”€ embedding_generation/
â”œâ”€â”€ embeddings.joblib
â”œâ”€â”€ inference.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

ğŸ¯ Future Improvements

Replace joblib with FAISS / Chroma

Add UI (Streamlit / FastAPI)

Support multiple videos

Add citation highlighting in answers

ğŸ‘¨â€ğŸ’» Author

Siddiqui Atif Iqbal
Data Science & Machine Learning Enthusiast
